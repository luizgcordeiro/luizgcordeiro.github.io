{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3eb099-ceb6-43ef-a135-a493d0301c4d",
   "metadata": {},
   "source": [
    "# 3.2 Asymptotic notation: formal definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992077c-0cc3-4dc3-b156-ccff7f4f577d",
   "metadata": {},
   "source": [
    "(3.2notation)=\n",
    "## Remarks on abuses of notation\n",
    "\n",
    "We should make a few considerations of $O$-notation, $\\Omega$-notation and $\\Theta$-notation, as the abuse of writing a formula in place of a function leads to some confusion.\n",
    "\n",
    "We denote by $\\mathbb{Z}=\\left\\{\\ldots,-2,-1,0,1,2,\\ldots\\right\\}$ the set of natural numbers, and $\\mathbb{R}$ the set of real numbers. Subscripts with unary predicates (possibly curried from many-ary predicates) are used to denote the restricted sets. So, for example, $\\mathbb{Z}_{\\geq 0} = \\left\\{n\\in\\mathbb{Z}\\mid n\\geq 0\\right\\}$ is a way to denote the set of ***natural numbers***. From now on, we only consider functions which are defined at least on a set of the form $\\mathbb{Z}_{\\geq a}$ for some $a\\in\\mathbb{Z}$ and which are asymptotically nonnegative (as defined in the book). As all definitions deal with limits at infinity, we do not need to worry too much about domains of functions being the same in our setting .\n",
    "\n",
    "First, recall that a function does not depend on any variable. So a proper mathematical definition of $O$-notation would be as follows:\n",
    "\n",
    "> Given a (asymptotically nonnegative) function $f$, we let\n",
    "> \\begin{equation*}O(f) = \\left\\{ g : \\limsup_{n\\to\\infty}\\dfrac{g(n)}{f(n)}<\\infty\\right\\}.\\end{equation*}\n",
    "\n",
    "The other asymptotic notation ($\\Omega$, $\\Theta$, $o$ and $\\omega$) are defined similarly.\n",
    "\n",
    "Of course, when there is no confusion, we may always specify a variable to be used for all formulas, and identify a function with a formula in the usual manner, and we will not refrain from doing this. However, this ought to be avoided when more than one variable is being used, as inconsistencies could arise. For example, as done in the book (p. 58), we allows ourselves to use $O(f)$ in place of a function $g\\in O(f)$. This is well and good for simple expressions. In particular, if we were to interpret the expression \"$\\sum_{i=1}^n O(i)$\", this ought to be interpreted as a function of the form\n",
    "\\begin{equation*}\\sum_{i=1}^n f_i,\\end{equation*}\n",
    "where $f_i\\in O(i)$ for each $i$. That is, $f_1\\in O(1)$, $f_2\\in O(2)$, $\\ldots$. However, the exact opposite is said in the book (p. 58), where it is stated that \"*in the expression $\\sum_{i=1}^n O(i)$ there is a single anonymous function (a function of $i$)*\".\n",
    "\n",
    "This problem arises from another convention: \"*The number of anonymous functions in an expression is understood to be equal to the number of times the asymptotic notation appears.*\". So, in fact, sometimes \"$O(f(n))$\" should be read as the **value** $g(n)$ for some $g\\in O(f)$ and some parameter $n$, and not as the function $g$. This ought to be more precisely written as $[O(f)](n)$.\n",
    "\n",
    "With this interpretation, $\\sum_{i=1}^n O(f(i))$ actually stands for $\\sum_{i=1}^n g(i)$ (which is a value depending on $n$) for some function $g\\in O(f)$. This applies in particular for $\\sum_{i=1}^n O(i)$, as long as we want this to denote the value (depending on $n$) of some function $g$, by taking $f$ the identity function in the previous phrase.\n",
    "\n",
    "Similar problems appear in items (f) and (g) of Problem 3-5, as the question statements are not well-formed (using undefined notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26f56e-8a53-4923-b94b-1f6df75560ea",
   "metadata": {},
   "source": [
    "## Remarks on running times\n",
    "\n",
    "Recall that ***running time*** is a notion that depends on an specific input, and not solely on the algorithm being executed (as is explicitly stated in p. 30). For an algorithm (or procedure) $A$ and a valid input $x$, we denote by $\\operatorname{time}(A,x)$ the time that algorithm $A$ takes to evaluate $A(x)$.\n",
    "\n",
    "However, we are interested in analysing running time with respect to some notion of \"size\" of an input, which itself depends on context: If an input is an array (as in sorting algorithms), the \"size\" is (usually) the length of the array; if the input is an integer (as in an algorithm which checks whether a number is prime or not), its \"size\" could be its absolute value.\n",
    "\n",
    "So, to properly define \"*running time*\", we need a notion of \"size\", which should be clear (even if implicit) from context. The following definitions are the ones used throughout the book:\n",
    "\n",
    "> The **worst-case running time** of an algorithm $A$ is the function\n",
    "> \\begin{equation*}n\\longmapsto \\sup\\left\\{\\operatorname{time}(A,x):x\\text{ has size }n\\right\\},\\end{equation*}\n",
    "> and the **best-case running time** of $A$ is the function\n",
    "> \\begin{equation*}n\\longmapsto \\inf\\left\\{\\operatorname{time}(A,x):x\\text{ has size }n\\right\\}.\\end{equation*}\n",
    ">\n",
    "> We say that algorithm $A$ has **running time $O(f)$** if its *worst*-case running time is so. Similarly, algorithm $A$ has **running time $\\Omega(f)$** if its *best*-case running time is so, and **running time $\\Theta(f)$** if it has both running time $O(f)$ and $\\Omega(f)$.\n",
    "\n",
    "Let us finish this discussion with a simple theorem. It has obvious analogues for $\\Omega$ and $\\Theta$ asymptotics, which we ommit.\n",
    "\n",
    "> **Theorem**: Algorithm $A$ has running time $\\Omega(f)$ if and only if for any sequence of inputs $(x_n)_{n=1}^\\infty$ with each $x_n$ of size $n$, we have $\\operatorname{time}(A,x_n)=\\Omega(f(n))$.\n",
    "\n",
    "Indeed, the \"only if\" part is immediate from the definition of supremum, so we take care of the \"if\" part. Assume that the latter property is true. Let $w\\colon n\\mapsto w(n)$ denote the worst-case running time function (with paremeter an input size $n$).\n",
    "\n",
    "First, note that we can assume that $f(n)>0$ for all sufficiently large $n$. If this is not the case, then there are infinitely many integers $n$ such that $f(n)=0$. Then for all sufficiently large such $n$ and all choices $x_n$ of inputs of sizes $n$ we have $\\operatorname{time}(A,x_n)=0$, by the definition of $O(f)$, from which it follows that $w(n)=0$ for all such $n$ as well. Thus, we can simply ignore such $n$ for which $f(n)=0$.\n",
    "\n",
    "Similarly, we can also assume that $w(n)<\\infty$ for all $n$, since this implies $f(n)=\\infty$ as well for sufficiently large $n$.\n",
    "\n",
    "For any $n\\in\\mathbb{N}$, choose an input $x_n$ of size $n$ such that $\\operatorname{time}(x_n)>w(n)-\\dfrac{f(n)}{n}$. Then\n",
    "\n",
    "\\begin{align*}\n",
    "\\limsup_{n\\to\\infty}\\dfrac{w(n)}{f(n)}\n",
    "  & \\leq\\dfrac{\\operatorname{time}(x_n)+f(n)/n}{f(n)}\\\\\n",
    "  &= \\dfrac{\\operatorname{time}(x_n)}{f(n)}+\\dfrac{1}{n} \n",
    "  \\\\&<\\infty.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $w(n)=\\Omega(f(n))$, which means that $A$ has running time $O(f)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751cd1b-f2a7-475a-adb2-7d72e300411e",
   "metadata": {},
   "source": [
    "## 3.2-1\n",
    "\n",
    "> Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that $\\max\\left\\{f(n),g(n)\\right\\} = \\Theta(f(n)+g(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cebe54-dd26-4414-863d-77d7bb20e72e",
   "metadata": {},
   "source": [
    "For large $n$,\n",
    "\\begin{equation*}\\max\\{f(n),g(n)\\}\\leq f(n)+g(n)\\leq 2*\\max\\{f(n),g(n)\\}.\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a1475-9b7e-42f8-93f6-6ea7bb88a1e6",
   "metadata": {},
   "source": [
    "## 3.2-2\n",
    "\n",
    "> Explain why the statement \"The running time of algorithm $A$ is at least $O(n^2)$\" is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fa72c-35a0-4aa2-84e3-6b119da1346e",
   "metadata": {},
   "source": [
    "The dry answer is that the phrase \"$f$ is at least $O(g)$\" ($f$ being a function, e.g. the running time of algorithm $A$) has never been defined and does not make sense, not even in terms of its components. So there we go: it is meaningless.\n",
    "\n",
    "If we want a more interesting answer, we can recall that, in common usage, saying that \"*$x$ is at least $y$*\" means that $x$ is greater than $y$ in some sense depending on context. As we are comparing functions asymptotically, to say that \"_$f$ is at least $O(g)$_\" should mean that \"$f\\geq O(g)$\", or that \"_$f\\geq g'$ for some $g'\\in O(g)$_\" (again using the convention that \"$O(g)$\" denotes an anonymous function in this class).\n",
    "\n",
    "However, ***every*** function satisfies this: simply take $g'=0$. So the phrase is vacuous and useless with this interpretation.\n",
    "\n",
    "This is in contrast to what happens if we use the same reasoning to $\\Omega$, which becomes meaningful (although unnecessary): Saying that \"*$f$ is at least $\\Omega(g)$*\" ought to mean that \"*$f\\geq g'$ for some $g'\\in\\Omega(g)$*\", which - by transitivity - simply means that $f\\in\\Omega(g)$. This is exactly what the book means by saying that \"_$f$ grows **at least as fast** as a certain rate_\" (in this case, the rate of growth of $g$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79783398-8734-4b0c-9a6c-83381683b403",
   "metadata": {},
   "source": [
    "## 3.2-3\n",
    "\n",
    "> Is $2^{n+1}=O(2^n)$? Is $2^{2^n}=O(2^n)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222a68e-027d-40f6-a2a5-cdf6e225de0f",
   "metadata": {},
   "source": [
    "We have\n",
    "\\begin{equation*}\\limsup_{n\\to\\infty}\\dfrac{2^{n+1}}{2^n}=\\limsup_{n\\to\\infty}2 = 2<\\infty,\\end{equation*}\n",
    "so $2^{n+1}=O(2^n)$. (Actually, $2^{n+1}=\\Theta(2^n)$.)\n",
    "\n",
    "As for the second part,\n",
    "\\begin{equation*}\\limsup_{n\\to\\infty}\\dfrac{2^{2^n}}{2^n}=\\limsup_{n\\to\\infty}2^{2^n-n}=\\infty,\\end{equation*}\n",
    "as $\\lim_{n\\to\\infty}2^n-n=\\infty$. So $2^{2^n}\\neq O(2^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b307972-d494-4c52-a25c-fe8e1ec91ec5",
   "metadata": {},
   "source": [
    "## 3.2-4\n",
    "\n",
    "> Prove Theorem 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0def40-7e0e-4512-a266-15f83de9ef70",
   "metadata": {},
   "source": [
    "This is immediate from the definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694e19a-d61f-419a-a23e-18df23f9f30f",
   "metadata": {},
   "source": [
    "## 3.2-5\n",
    "\n",
    "> Prove that the running time of an algorithm if $\\Theta(g(n))$ if and only if its worst-case running times is $O(g(n))$ and its best-case running time is $\\Omega(g(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d15fb-8e32-4c59-8167-341b3c327bd3",
   "metadata": {},
   "source": [
    "This was discussed above, before the exercise solutions, as \"running time\" of an algorithm is not really well-defined in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10ff2f-7326-4d04-8d63-3dc0bf0f8df1",
   "metadata": {},
   "source": [
    "## 3.2-6\n",
    "\n",
    "> Prove that $o(g(n))\\cap \\omega(g(n))$ is the empty set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02940c69-37c1-44ea-aa09-ee90b0afb089",
   "metadata": {},
   "source": [
    "We can do even better: $O(g(n))\\cap \\omega(g(n))$ is the empty set (and, similarly, $o(g(n))\\cap \\Omega(g(n))$ is also the empty set).\n",
    "\n",
    "Indeed, if $f(n)=O(g(n))$, then there exists $c>0$ such that, for sufficiently large $n$,\n",
    "\\begin{equation*}f(n)\\leq cg(n).\\end{equation*}\n",
    "But if also $f(n)=\\omega(g(n))$, then also for all sufficiently large $n$ we have\n",
    "\\begin{equation*}c g(n) < f(n),\\end{equation*}\n",
    "which is a contradiction to the previous inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee440ad6-b092-44bd-84ce-1ec62a084ad4",
   "metadata": {},
   "source": [
    "## 3.2-7\n",
    "\n",
    "> We can extend our notation to the case of two parameters $n$ and $m$ that can go to $\\infty$ independently at different rates. For a given function $g(n,m)$, we denote by $O(g(n,m))$ the set of functions\n",
    "> \\begin{equation*}\\begin{array}{rcl}\n",
    "    O(g(n,m))=\\left\\{ f(n,m) \\right.&:\n",
    "        &\\text{there exist positive constants} c, n_0, \\text{ and }m_0\\\\\n",
    "        &&\\text{such that }0 \\leq f(n,m) \\leq c g(n,m)\\\\\n",
    "        &&\\left.\\text{for all }n\\geq n_0\\text{ or }m\\geq m_0\\right\\}.\\end{array}\\end{equation*}\n",
    "> Give corresponding definitions for $\\Omega(g(n,m))$ and $\\Theta(g(n,m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4937340-af4e-4d57-9e82-abe7c6b86660",
   "metadata": {},
   "source": [
    "\\begin{equation*}\\begin{array}{rcl}\n",
    "    \\Omega(g(n,m))=\\left\\{ f(n,m) \\right.&:\n",
    "        &\\text{there exist positive constants} c, n_0, \\text{ and }m_0\\\\\n",
    "        &&\\text{such that }0 \\leq c g(n,m) \\leq f(n,m)\\\\\n",
    "        &&\\left.\\text{for all }n\\geq n_0\\text{ or }m\\geq m_0\\right\\},\\end{array}\\end{equation*}\n",
    "and $\\Theta(g(n,m))=O(g(n,m))\\cap\\Omega(g(n,m))$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C",
   "language": "c",
   "name": "c"
  },
  "language_info": {
   "file_extension": ".c",
   "mimetype": "text/x-csrc",
   "name": "text/x-csrc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
